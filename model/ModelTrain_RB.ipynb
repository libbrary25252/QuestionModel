{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['String', 'Algorithm Design', 'Basic Machine Organisation', 'Computer System', 'Data Manipulation and Analysis', 'Data Organisation and Data Control', 'Elementary Web Authoring', 'Health and Ethical Issues', 'Information Processing', 'Intellectual Property', 'Internet Services and Applications', 'Multimedia Elements', 'Networking and Internet Basics', 'Program Development', 'Spreadsheets and Databases', 'Threats and Security on the Internet', '__index_level_0__'],\n",
       "        num_rows: 644\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['String', 'Algorithm Design', 'Basic Machine Organisation', 'Computer System', 'Data Manipulation and Analysis', 'Data Organisation and Data Control', 'Elementary Web Authoring', 'Health and Ethical Issues', 'Information Processing', 'Intellectual Property', 'Internet Services and Applications', 'Multimedia Elements', 'Networking and Internet Basics', 'Program Development', 'Spreadsheets and Databases', 'Threats and Security on the Internet', '__index_level_0__'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['String', 'Algorithm Design', 'Basic Machine Organisation', 'Computer System', 'Data Manipulation and Analysis', 'Data Organisation and Data Control', 'Elementary Web Authoring', 'Health and Ethical Issues', 'Information Processing', 'Intellectual Property', 'Internet Services and Applications', 'Multimedia Elements', 'Networking and Internet Basics', 'Program Development', 'Spreadsheets and Databases', 'Threats and Security on the Internet', '__index_level_0__'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training\n",
    "from datasets import load_from_disk\n",
    "\n",
    "datasets = load_from_disk(\"./encoded_data4\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['String', 'Algorithm Design', 'Basic Machine Organisation', 'Computer System', 'Data Manipulation and Analysis', 'Data Organisation and Data Control', 'Elementary Web Authoring', 'Health and Ethical Issues', 'Information Processing', 'Intellectual Property', 'Internet Services and Applications', 'Multimedia Elements', 'Networking and Internet Basics', 'Program Development', 'Spreadsheets and Databases', 'Threats and Security on the Internet'],\n",
       "        num_rows: 644\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['String', 'Algorithm Design', 'Basic Machine Organisation', 'Computer System', 'Data Manipulation and Analysis', 'Data Organisation and Data Control', 'Elementary Web Authoring', 'Health and Ethical Issues', 'Information Processing', 'Intellectual Property', 'Internet Services and Applications', 'Multimedia Elements', 'Networking and Internet Basics', 'Program Development', 'Spreadsheets and Databases', 'Threats and Security on the Internet'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['String', 'Algorithm Design', 'Basic Machine Organisation', 'Computer System', 'Data Manipulation and Analysis', 'Data Organisation and Data Control', 'Elementary Web Authoring', 'Health and Ethical Issues', 'Information Processing', 'Intellectual Property', 'Internet Services and Applications', 'Multimedia Elements', 'Networking and Internet Basics', 'Program Development', 'Spreadsheets and Databases', 'Threats and Security on the Internet'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labels = [label for label in datasets['train'].features.keys() if label not in ['Unnamed: 0', 'String', '__index_level_0__']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Algorithm Design',\n",
       " 'Basic Machine Organisation',\n",
       " 'Computer System',\n",
       " 'Data Manipulation and Analysis',\n",
       " 'Data Organisation and Data Control',\n",
       " 'Elementary Web Authoring',\n",
       " 'Health and Ethical Issues',\n",
       " 'Information Processing',\n",
       " 'Intellectual Property',\n",
       " 'Internet Services and Applications',\n",
       " 'Multimedia Elements',\n",
       " 'Networking and Internet Basics',\n",
       " 'Program Development',\n",
       " 'Spreadsheets and Databases',\n",
       " 'Threats and Security on the Internet']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['String',\n",
       " 'Algorithm Design',\n",
       " 'Basic Machine Organisation',\n",
       " 'Computer System',\n",
       " 'Data Manipulation and Analysis',\n",
       " 'Data Organisation and Data Control',\n",
       " 'Elementary Web Authoring',\n",
       " 'Health and Ethical Issues',\n",
       " 'Information Processing',\n",
       " 'Intellectual Property',\n",
       " 'Internet Services and Applications',\n",
       " 'Multimedia Elements',\n",
       " 'Networking and Internet Basics',\n",
       " 'Program Development',\n",
       " 'Spreadsheets and Databases',\n",
       " 'Threats and Security on the Internet']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'String': Value(dtype='string', id=None),\n",
       " 'Algorithm Design': Value(dtype='int32', id=None),\n",
       " 'Basic Machine Organisation': Value(dtype='int32', id=None),\n",
       " 'Computer System': Value(dtype='int32', id=None),\n",
       " 'Data Manipulation and Analysis': Value(dtype='int32', id=None),\n",
       " 'Data Organisation and Data Control': Value(dtype='int32', id=None),\n",
       " 'Elementary Web Authoring': Value(dtype='int32', id=None),\n",
       " 'Health and Ethical Issues': Value(dtype='int32', id=None),\n",
       " 'Information Processing': Value(dtype='int32', id=None),\n",
       " 'Intellectual Property': Value(dtype='int32', id=None),\n",
       " 'Internet Services and Applications': Value(dtype='int32', id=None),\n",
       " 'Multimedia Elements': Value(dtype='int32', id=None),\n",
       " 'Networking and Internet Basics': Value(dtype='int32', id=None),\n",
       " 'Program Development': Value(dtype='int32', id=None),\n",
       " 'Spreadsheets and Databases': Value(dtype='int32', id=None),\n",
       " 'Threats and Security on the Internet': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"{datasets.train.column_names}\\n{datasets.train.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Algorithm Design',\n",
       " 'Basic Machine Organisation',\n",
       " 'Computer System',\n",
       " 'Data Manipulation and Analysis',\n",
       " 'Data Organisation and Data Control',\n",
       " 'Elementary Web Authoring',\n",
       " 'Health and Ethical Issues',\n",
       " 'Information Processing',\n",
       " 'Intellectual Property',\n",
       " 'Internet Services and Applications',\n",
       " 'Multimedia Elements',\n",
       " 'Networking and Internet Basics',\n",
       " 'Program Development',\n",
       " 'Spreadsheets and Databases',\n",
       " 'Threats and Security on the Internet']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {idx:label for idx, label in enumerate(model_labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(model_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--Zamachi--RoBERTa-for-multilabel-sentence-classification\\snapshots\\f8d542318fae3affd54b48577eab964e700d3f72\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--Zamachi--RoBERTa-for-multilabel-sentence-classification\\snapshots\\f8d542318fae3affd54b48577eab964e700d3f72\\merges.txt\n",
      "loading file added_tokens.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--Zamachi--RoBERTa-for-multilabel-sentence-classification\\snapshots\\f8d542318fae3affd54b48577eab964e700d3f72\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--Zamachi--RoBERTa-for-multilabel-sentence-classification\\snapshots\\f8d542318fae3affd54b48577eab964e700d3f72\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--Zamachi--RoBERTa-for-multilabel-sentence-classification\\snapshots\\f8d542318fae3affd54b48577eab964e700d3f72\\tokenizer_config.json\n",
      "Adding 😍 to the vocabulary\n",
      "Adding 😂 to the vocabulary\n",
      "Adding 💕 to the vocabulary\n",
      "Adding 🔥 to the vocabulary\n",
      "Adding 😊 to the vocabulary\n",
      "Adding 😎 to the vocabulary\n",
      "Adding ✨ to the vocabulary\n",
      "Adding 💙 to the vocabulary\n",
      "Adding 😘 to the vocabulary\n",
      "Adding 📷 to the vocabulary\n",
      "Adding 🇺🇸 to the vocabulary\n",
      "Adding ☀ to the vocabulary\n",
      "Adding 💜 to the vocabulary\n",
      "Adding 😉 to the vocabulary\n",
      "Adding 💯 to the vocabulary\n",
      "Adding 😁 to the vocabulary\n",
      "Adding 🎄 to the vocabulary\n",
      "Adding 📸 to the vocabulary\n",
      "Adding 😜 to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Zamachi/RoBERTa-for-multilabel-sentence-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(dataset):\n",
    "    text = dataset[\"String\"]\n",
    "    # tokenize string\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=70)\n",
    "    # create encoded array list with labels\n",
    "    labelsBatch = {x: dataset[x] for x in dataset.keys() if x in model_labels}\n",
    "    labels_matrix = np.zeros((len(text), len(model_labels)))\n",
    "    for idx, label in enumerate(model_labels):\n",
    "        labels_matrix[:, idx] = labelsBatch[label]\n",
    "    \n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at c:\\Users\\user\\Desktop\\QuestionQuestionModel\\model\\encoded_data4\\train\\cache-9cdeced0d371d054.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.84ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.78ba/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "encoded_ds = datasets.map(encode_data, batched=True, remove_columns=datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ds['train']['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Organisation and Data Control']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(encoded_ds['train'][15]['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "encoded_ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 644\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--Zamachi--RoBERTa-for-multilabel-sentence-classification\\snapshots\\f8d542318fae3affd54b48577eab964e700d3f72\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"c:\\\\Users\\\\Zamachi\\\\Projects\\\\Finals\\\\RoBERTa-for-multilabel-sentence-classification\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Algorithm Design\",\n",
      "    \"1\": \"Basic Machine Organisation\",\n",
      "    \"2\": \"Computer System\",\n",
      "    \"3\": \"Data Manipulation and Analysis\",\n",
      "    \"4\": \"Data Organisation and Data Control\",\n",
      "    \"5\": \"Elementary Web Authoring\",\n",
      "    \"6\": \"Health and Ethical Issues\",\n",
      "    \"7\": \"Information Processing\",\n",
      "    \"8\": \"Intellectual Property\",\n",
      "    \"9\": \"Internet Services and Applications\",\n",
      "    \"10\": \"Multimedia Elements\",\n",
      "    \"11\": \"Networking and Internet Basics\",\n",
      "    \"12\": \"Program Development\",\n",
      "    \"13\": \"Spreadsheets and Databases\",\n",
      "    \"14\": \"Threats and Security on the Internet\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Algorithm Design\": 0,\n",
      "    \"Basic Machine Organisation\": 1,\n",
      "    \"Computer System\": 2,\n",
      "    \"Data Manipulation and Analysis\": 3,\n",
      "    \"Data Organisation and Data Control\": 4,\n",
      "    \"Elementary Web Authoring\": 5,\n",
      "    \"Health and Ethical Issues\": 6,\n",
      "    \"Information Processing\": 7,\n",
      "    \"Intellectual Property\": 8,\n",
      "    \"Internet Services and Applications\": 9,\n",
      "    \"Multimedia Elements\": 10,\n",
      "    \"Networking and Internet Basics\": 11,\n",
      "    \"Program Development\": 12,\n",
      "    \"Spreadsheets and Databases\": 13,\n",
      "    \"Threats and Security on the Internet\": 14\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50284\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\user/.cache\\huggingface\\hub\\models--Zamachi--RoBERTa-for-multilabel-sentence-classification\\snapshots\\f8d542318fae3affd54b48577eab964e700d3f72\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Zamachi/RoBERTa-for-multilabel-sentence-classification and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([15]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"Zamachi/RoBERTa-for-multilabel-sentence-classification\", problem_type=\"multi_label_classification\", num_labels=len(model_labels), id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric  = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "train_name_dir = time.strftime(f\"%Y%m%d_%H%M%S$-train-model-{model.name_or_path}\")\n",
    "train_name_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./RBert-v1/\"+ train_name_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs= 81,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='./RBert-v1/logging/'+ train_name_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2e-05"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "# optimizer = AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score, hamming_loss\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def multi_label_metrics(predictions, labels, threshold=0.50):\n",
    "    # apply sigmoid on predictions fitting (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    # logits, labels = predictions\n",
    "    # prediction = np.argmax(logits, axis=-1)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy,\n",
    "               'balanced_accuracy': balanced_accuracy,\n",
    "               }\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "def compute_metrics_new(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ds['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7616, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[ 0.4198, -0.0892,  0.0682, -0.0094,  0.1213,  0.1981, -0.1380, -0.2670,\n",
       "          0.4694,  0.2932, -0.2617, -0.0059,  0.2165,  0.0412,  0.2677]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=encoded_ds['train']['input_ids'][0].unsqueeze(0), labels=encoded_ds['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_ds[\"train\"],\n",
    "    eval_dataset=encoded_ds[\"valid\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 644\n",
      "  Num Epochs = 81\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6561\n",
      "  Number of trainable parameters = 124670991\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "                                                   \n",
      "\n",
      "\u001b[A\u001b[A                                        \n",
      " 98%|█████████▊| 6399/6561 [47:26<00:14, 11.20it/s]\n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-81\n",
      "Configuration saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-81\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2531745731830597, 'eval_f1': 0.0, 'eval_roc_auc': 0.5, 'eval_accuracy': 0.0, 'eval_balanced_accuracy': 0.06666666666666667, 'eval_runtime': 0.2433, 'eval_samples_per_second': 332.944, 'eval_steps_per_second': 45.215, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-81\\pytorch_model.bin\n",
      "tokenizer config file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-81\\tokenizer_config.json\n",
      "Special tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-81\\special_tokens_map.json\n",
      "added tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-81\\added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                               \n",
      "\n",
      " 98%|█████████▊| 6399/6561 [47:44<00:14, 11.20it/s]\n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-162\n",
      "Configuration saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-162\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2535138726234436, 'eval_f1': 0.0, 'eval_roc_auc': 0.5, 'eval_accuracy': 0.0, 'eval_balanced_accuracy': 0.06666666666666667, 'eval_runtime': 0.2221, 'eval_samples_per_second': 364.737, 'eval_steps_per_second': 49.532, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-162\\pytorch_model.bin\n",
      "tokenizer config file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-162\\tokenizer_config.json\n",
      "Special tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-162\\special_tokens_map.json\n",
      "added tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-162\\added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                               \n",
      "\n",
      " 98%|█████████▊| 6399/6561 [48:00<00:14, 11.20it/s]\n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-243\n",
      "Configuration saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-243\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25740471482276917, 'eval_f1': 0.0, 'eval_roc_auc': 0.5, 'eval_accuracy': 0.0, 'eval_balanced_accuracy': 0.06666666666666667, 'eval_runtime': 0.2254, 'eval_samples_per_second': 359.387, 'eval_steps_per_second': 48.806, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-243\\pytorch_model.bin\n",
      "tokenizer config file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-243\\tokenizer_config.json\n",
      "Special tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-243\\special_tokens_map.json\n",
      "added tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-243\\added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                               \n",
      "\n",
      " 98%|█████████▊| 6399/6561 [48:24<00:14, 11.20it/s]\n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-324\n",
      "Configuration saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-324\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25654149055480957, 'eval_f1': 0.0, 'eval_roc_auc': 0.5, 'eval_accuracy': 0.0, 'eval_balanced_accuracy': 0.06666666666666667, 'eval_runtime': 0.2344, 'eval_samples_per_second': 345.505, 'eval_steps_per_second': 46.92, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-324\\pytorch_model.bin\n",
      "tokenizer config file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-324\\tokenizer_config.json\n",
      "Special tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-324\\special_tokens_map.json\n",
      "added tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-324\\added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                               \n",
      "\n",
      " 98%|█████████▊| 6399/6561 [48:34<00:14, 11.20it/s]\n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-405\n",
      "Configuration saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-405\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2560178339481354, 'eval_f1': 0.0, 'eval_roc_auc': 0.5, 'eval_accuracy': 0.0, 'eval_balanced_accuracy': 0.06666666666666667, 'eval_runtime': 0.227, 'eval_samples_per_second': 356.887, 'eval_steps_per_second': 48.466, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-405\\pytorch_model.bin\n",
      "tokenizer config file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-405\\tokenizer_config.json\n",
      "Special tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-405\\special_tokens_map.json\n",
      "added tokens file saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-405\\added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                               \n",
      "\n",
      " 98%|█████████▊| 6399/6561 [48:58<00:14, 11.20it/s]\n",
      "\u001b[A\n",
      "\u001b[ASaving model checkpoint to ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-486\n",
      "Configuration saved in ./RBert-v1/20230329_213955$-train-model-Zamachi/RoBERTa-for-multilabel-sentence-classification\\checkpoint-486\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26556873321533203, 'eval_f1': 0.17699115044247787, 'eval_roc_auc': 0.5506107483469628, 'eval_accuracy': 0.09876543209876543, 'eval_balanced_accuracy': 0.08888888888888888, 'eval_runtime': 0.2289, 'eval_samples_per_second': 353.812, 'eval_steps_per_second': 48.049, 'epoch': 6.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\transformers\\trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1542\u001b[0m )\n\u001b[1;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1548\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\transformers\\trainer.py:1883\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1880\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1882\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 1883\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1885\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[0;32m   1886\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   1887\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\transformers\\trainer.py:2135\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m-> 2135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[0;32m   2136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\transformers\\trainer.py:2192\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2190\u001b[0m run_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_dir(trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m   2191\u001b[0m output_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[1;32m-> 2192\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_model(output_dir, _internal_call\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   2193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[39m# config `stage3_gather_16bit_weights_on_model_save` is True\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\transformers\\trainer.py:2671\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[1;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n\u001b[0;32m   2670\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m-> 2671\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save(output_dir)\n\u001b[0;32m   2673\u001b[0m \u001b[39m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[0;32m   2674\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\transformers\\trainer.py:2723\u001b[0m, in \u001b[0;36mTrainer._save\u001b[1;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[0;32m   2721\u001b[0m         torch\u001b[39m.\u001b[39msave(state_dict, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[0;32m   2722\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave_pretrained(output_dir, state_dict\u001b[39m=\u001b[39;49mstate_dict)\n\u001b[0;32m   2724\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2725\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:1701\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, **kwargs)\u001b[0m\n\u001b[0;32m   1699\u001b[0m         safe_save_file(shard, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m   1700\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1701\u001b[0m         save_function(shard, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_directory, shard_file))\n\u001b[0;32m   1703\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1704\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel weights saved in \u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory,\u001b[39m \u001b[39mWEIGHTS_NAME)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\torch\\serialization.py:423\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    422\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\QuestionQuestionModel\\venv\\lib\\site-packages\\torch\\serialization.py:650\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    649\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n\u001b[1;32m--> 650\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(name, storage\u001b[39m.\u001b[39mdata_ptr(), num_bytes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result = trainer.predict(encoded_ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./RBert-model-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./RBert-model-v1/tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e32da21edddcaf775c480ccd7b569f19fc9b1c5c20d607727913c5339e7adca8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
